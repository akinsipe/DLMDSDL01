{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.2"},"colab":{"name":"IrisKerasCallback.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"Q6Lh_k1lXV_9","colab_type":"text"},"source":["# A Simple Network in Keras with Callbacks\n","\n","TensorFlow provides callbacks that can be used to extend the functionality of the training loop.\n","\n","In this notebook we explore a few examples.\n","\n","We have previously seen how we can store metrics such as the loss function or the accuracy in\n","the history object returned by the ```fit()``` method.\n","TensorFlow provides a much more detailed system for monitoring and analyzing training sessions\n","as well as model performance which is called [TensorBoard](https://www.tensorflow.org/tensorboard).\n","To be able to use this functionality, the special \"hooks\" or functions have to be added\n","to the training loops that take care of logging the relevant information. This is done as a \"callback\".\n","\n","Another useful callback to add are model checkpoints. Training complex neural networks may take a\n","long time and if for example the computer crashes during training, all training progress is lost.\n","However, we can add another callback to save intermiate trainning progress at regular intervals.\n","Should something happen, we can then resume the training from this point onwards.\n","\n","In this example, we use the [Iris dataset](http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html)\n","again with a simple network we have used earlier."]},{"cell_type":"code","metadata":{"id":"TFAV-VLyXV_-","colab_type":"code","colab":{},"outputId":"d19bc9f5-00b1-4913-b361-d9f5105222f6"},"source":["# import required libraries\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import datetime as dt\n","import tensorflow as tf\n","from sklearn.datasets import load_iris\n","from sklearn.metrics import confusion_matrix\n","import pydot\n","\n","%pylab inline\n","# large figures\n","rcParams['figure.figsize'] = 8, 6\n","\n","print(tf.__version__)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Populating the interactive namespace from numpy and matplotlib\n","2.2.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cHel-2WzXWAD","colab_type":"code","colab":{}},"source":["#load the data\n","iris = load_iris()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n8K2GsqwXWAH","colab_type":"code","colab":{},"outputId":"4045f53d-eef9-49f7-bd28-a4836729fa32"},"source":["#define the network model\n","n_input = 4\n","n_output = 3\n","model = tf.keras.Sequential()\n","model.add(tf.keras.layers.Dense(10, activation=tf.nn.relu, input_shape=(n_input,)))\n","model.add(tf.keras.layers.Dense(10, activation=tf.nn.relu))\n","model.add(tf.keras.layers.Dense(n_output))\n","model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense (Dense)                (None, 10)                50        \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 10)                110       \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 3)                 33        \n","=================================================================\n","Total params: 193\n","Trainable params: 193\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4_t5eJD7XWAK","colab_type":"code","colab":{}},"source":["# setup the model\n","model.compile(\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01),\n","    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","    metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5WqCSRLmXWAM","colab_type":"text"},"source":["Now we setup the callbacks.\n","Note that the model checkpoints make the training quite slow as the checkpoints have to be written to disk. Hence we only make a checkpoint every few training iterations (i.e. period or epoch)"]},{"cell_type":"code","metadata":{"id":"f0vzvIfrXWAN","colab_type":"code","colab":{},"outputId":"a2fe37fc-1d41-4eff-b2f5-a10c9a3d8405"},"source":["callbacks = [\n","    # write logs to ./logs\n","    tf.keras.callbacks.TensorBoard(log_dir='./logs/{}'.format(dt.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"))),\n","    # write checkpoints to ./checkpoints\n","    tf.keras.callbacks.ModelCheckpoint(filepath='./checkpoints/model{epoch:02d}.h5', save_freq='epoch'),\n","]\n","\n","history = model.fit(\n","        x=iris.data,\n","        y=iris.target,\n","        batch_size=32,\n","        epochs=50,\n","        verbose=1,\n","        callbacks=callbacks\n","    )"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/50\n","5/5 [==============================] - 0s 27ms/step - loss: 0.9999 - sparse_categorical_accuracy: 0.3333\n","Epoch 2/50\n","5/5 [==============================] - 0s 10ms/step - loss: 0.8366 - sparse_categorical_accuracy: 0.6533\n","Epoch 3/50\n","5/5 [==============================] - 0s 10ms/step - loss: 0.6939 - sparse_categorical_accuracy: 0.8333\n","Epoch 4/50\n","5/5 [==============================] - 0s 11ms/step - loss: 0.5744 - sparse_categorical_accuracy: 0.8333\n","Epoch 5/50\n","5/5 [==============================] - 0s 10ms/step - loss: 0.4812 - sparse_categorical_accuracy: 0.8800\n","Epoch 6/50\n","5/5 [==============================] - 0s 10ms/step - loss: 0.4162 - sparse_categorical_accuracy: 0.8867\n","Epoch 7/50\n","5/5 [==============================] - 0s 10ms/step - loss: 0.3845 - sparse_categorical_accuracy: 0.9000\n","Epoch 8/50\n","5/5 [==============================] - 0s 10ms/step - loss: 0.3505 - sparse_categorical_accuracy: 0.9133\n","Epoch 9/50\n","5/5 [==============================] - 0s 14ms/step - loss: 0.3333 - sparse_categorical_accuracy: 0.8400\n","Epoch 10/50\n","5/5 [==============================] - 0s 11ms/step - loss: 0.3010 - sparse_categorical_accuracy: 0.9333\n","Epoch 11/50\n","5/5 [==============================] - 0s 10ms/step - loss: 0.2670 - sparse_categorical_accuracy: 0.9600\n","Epoch 12/50\n","5/5 [==============================] - 0s 13ms/step - loss: 0.2505 - sparse_categorical_accuracy: 0.9467\n","Epoch 13/50\n","5/5 [==============================] - 0s 16ms/step - loss: 0.2401 - sparse_categorical_accuracy: 0.9533\n","Epoch 14/50\n","5/5 [==============================] - 0s 13ms/step - loss: 0.2233 - sparse_categorical_accuracy: 0.9400\n","Epoch 15/50\n","5/5 [==============================] - 0s 15ms/step - loss: 0.2187 - sparse_categorical_accuracy: 0.9533\n","Epoch 16/50\n","5/5 [==============================] - 0s 13ms/step - loss: 0.1846 - sparse_categorical_accuracy: 0.9467\n","Epoch 17/50\n","5/5 [==============================] - 0s 12ms/step - loss: 0.1756 - sparse_categorical_accuracy: 0.9533\n","Epoch 18/50\n","5/5 [==============================] - 0s 12ms/step - loss: 0.1631 - sparse_categorical_accuracy: 0.9533\n","Epoch 19/50\n","5/5 [==============================] - 0s 13ms/step - loss: 0.1504 - sparse_categorical_accuracy: 0.9600\n","Epoch 20/50\n","5/5 [==============================] - 0s 13ms/step - loss: 0.1417 - sparse_categorical_accuracy: 0.9600\n","Epoch 21/50\n","5/5 [==============================] - 0s 12ms/step - loss: 0.1380 - sparse_categorical_accuracy: 0.9533\n","Epoch 22/50\n","5/5 [==============================] - 0s 15ms/step - loss: 0.1312 - sparse_categorical_accuracy: 0.9467\n","Epoch 23/50\n","5/5 [==============================] - 0s 31ms/step - loss: 0.1223 - sparse_categorical_accuracy: 0.9600\n","Epoch 24/50\n","5/5 [==============================] - 0s 14ms/step - loss: 0.1168 - sparse_categorical_accuracy: 0.9533\n","Epoch 25/50\n","5/5 [==============================] - 0s 15ms/step - loss: 0.1081 - sparse_categorical_accuracy: 0.9600\n","Epoch 26/50\n","5/5 [==============================] - 0s 12ms/step - loss: 0.1078 - sparse_categorical_accuracy: 0.9533\n","Epoch 27/50\n","5/5 [==============================] - 0s 12ms/step - loss: 0.1000 - sparse_categorical_accuracy: 0.9667\n","Epoch 28/50\n","5/5 [==============================] - 0s 10ms/step - loss: 0.0985 - sparse_categorical_accuracy: 0.9600\n","Epoch 29/50\n","5/5 [==============================] - 0s 13ms/step - loss: 0.0953 - sparse_categorical_accuracy: 0.9600\n","Epoch 30/50\n","5/5 [==============================] - 0s 11ms/step - loss: 0.1080 - sparse_categorical_accuracy: 0.9667\n","Epoch 31/50\n","5/5 [==============================] - 0s 11ms/step - loss: 0.1610 - sparse_categorical_accuracy: 0.9533\n","Epoch 32/50\n","5/5 [==============================] - 0s 12ms/step - loss: 0.1172 - sparse_categorical_accuracy: 0.9467\n","Epoch 33/50\n","5/5 [==============================] - 0s 13ms/step - loss: 0.1218 - sparse_categorical_accuracy: 0.9600\n","Epoch 34/50\n","5/5 [==============================] - 0s 12ms/step - loss: 0.1243 - sparse_categorical_accuracy: 0.9600\n","Epoch 35/50\n","5/5 [==============================] - 0s 15ms/step - loss: 0.1148 - sparse_categorical_accuracy: 0.9600\n","Epoch 36/50\n","5/5 [==============================] - 0s 12ms/step - loss: 0.1009 - sparse_categorical_accuracy: 0.9533\n","Epoch 37/50\n","5/5 [==============================] - 0s 12ms/step - loss: 0.0770 - sparse_categorical_accuracy: 0.9600\n","Epoch 38/50\n","5/5 [==============================] - 0s 14ms/step - loss: 0.0773 - sparse_categorical_accuracy: 0.9733\n","Epoch 39/50\n","5/5 [==============================] - 0s 13ms/step - loss: 0.1029 - sparse_categorical_accuracy: 0.9600\n","Epoch 40/50\n","5/5 [==============================] - 0s 12ms/step - loss: 0.0772 - sparse_categorical_accuracy: 0.9800\n","Epoch 41/50\n","5/5 [==============================] - 0s 11ms/step - loss: 0.0744 - sparse_categorical_accuracy: 0.9533\n","Epoch 42/50\n","5/5 [==============================] - 0s 10ms/step - loss: 0.0827 - sparse_categorical_accuracy: 0.9667\n","Epoch 43/50\n","5/5 [==============================] - 0s 14ms/step - loss: 0.0817 - sparse_categorical_accuracy: 0.9733\n","Epoch 44/50\n","5/5 [==============================] - 0s 11ms/step - loss: 0.0761 - sparse_categorical_accuracy: 0.9600\n","Epoch 45/50\n","5/5 [==============================] - 0s 13ms/step - loss: 0.0793 - sparse_categorical_accuracy: 0.9667\n","Epoch 46/50\n","5/5 [==============================] - 0s 11ms/step - loss: 0.0791 - sparse_categorical_accuracy: 0.9533\n","Epoch 47/50\n","5/5 [==============================] - 0s 11ms/step - loss: 0.0721 - sparse_categorical_accuracy: 0.9600\n","Epoch 48/50\n","5/5 [==============================] - 0s 13ms/step - loss: 0.0752 - sparse_categorical_accuracy: 0.9600\n","Epoch 49/50\n","5/5 [==============================] - 0s 11ms/step - loss: 0.0723 - sparse_categorical_accuracy: 0.9600\n","Epoch 50/50\n","5/5 [==============================] - 0s 13ms/step - loss: 0.0754 - sparse_categorical_accuracy: 0.9733\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"eAM3FpmkXWAP","colab_type":"text"},"source":["After training we can confirm that we have the model saved after each complete epoch\n","and we also have the relevant logs for TensorBoard."]},{"cell_type":"code","metadata":{"id":"rlzIJJuoXWAQ","colab_type":"code","colab":{},"outputId":"a464d379-90c9-4e27-b47c-09a1e1165c69"},"source":["!ls logs checkpoints\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["checkpoints:\r\n","empty.txt   model09.h5\tmodel18.h5  model27.h5\tmodel36.h5  model45.h5\r\n","model01.h5  model10.h5\tmodel19.h5  model28.h5\tmodel37.h5  model46.h5\r\n","model02.h5  model11.h5\tmodel20.h5  model29.h5\tmodel38.h5  model47.h5\r\n","model03.h5  model12.h5\tmodel21.h5  model30.h5\tmodel39.h5  model48.h5\r\n","model04.h5  model13.h5\tmodel22.h5  model31.h5\tmodel40.h5  model49.h5\r\n","model05.h5  model14.h5\tmodel23.h5  model32.h5\tmodel41.h5  model50.h5\r\n","model06.h5  model15.h5\tmodel24.h5  model33.h5\tmodel42.h5\r\n","model07.h5  model16.h5\tmodel25.h5  model34.h5\tmodel43.h5\r\n","model08.h5  model17.h5\tmodel26.h5  model35.h5\tmodel44.h5\r\n","\r\n","logs:\r\n","2020-08-21-10-17-40  2020-08-21-10-18-58  2020-08-21-10-23-38  empty.txt\r\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GDaUiWoLXWAS","colab_type":"text"},"source":["## Visualization with TensorBoard\n","\n","Before, during or after training we can then open TensorBoard. \n","From a command-line shell, we use ```tensorboard --logdir logs``` where ```logs``` is the directory containing all the log-files we\n","have specified when setting up the TensorBoard callback.\n","\n","We can then open a webbrowser and point it to the location where TensorBoard runs, e.g. ``` http://localhost:6006/```\n","\n","An example is shown below:\n","![TensorBoard Example](TensorBoard_Example_Iris_2.png)"]},{"cell_type":"markdown","metadata":{"id":"9uT-ADKPXWAT","colab_type":"text"},"source":["We can select the training runs we want to analyze or monitor live (in case the traning is progressing),\n","hover over the graphs with the mouse and obtain more details at a specific training step."]},{"cell_type":"code","metadata":{"id":"GxVuVdojXWAU","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}
